{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchnlp.encoders.text import SpacyEncoder, pad_tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - dropping useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "df = pd.read_csv('data/Reviews.csv')\n",
    "\n",
    "#drop useless data\n",
    "df = df.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
    "       'HelpfulnessDenominator', 'Time', 'Summary',], axis=1)\n",
    "\n",
    "#remove ambiguous 3 and 4 stars for balancing\n",
    "#\n",
    "df = df[df['Score'] != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create labels and preprocess\n",
    "df['Score'] = df['Score'].apply(lambda i: 'positive' if i > 4 else 'negative')\n",
    "df['Text'] = df['Text'].apply(lambda x:x.lower())\n",
    "\n",
    "#set names for beautiful df\n",
    "df.columns = ['labels', 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing = Balancing dataset and transforming into list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_positive = df[df['labels']=='positive'].index\n",
    "nbr_to_drop = len(df) - len(idx_positive)\n",
    "\n",
    "drop_indices = np.random.choice(idx_positive, nbr_to_drop, replace=False)\n",
    "df = df.drop(drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['labels'] =='negative').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_list = df['text'].tolist()\n",
    "labels_as_list = df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - Encoding texts to idx sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoder = SpacyEncoder(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "encoded_texts = []\n",
    "for i in tqdm(range(len(text_as_list))):\n",
    "    encoded_texts.append(encoder.encode(text_as_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lengths = [len(i) for i in tqdm(encoded_texts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - dropping too big texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_as_series = pd.Series(lengths)\n",
    "plt.title(\"Probability Density Function for text lengths\")\n",
    "sns.distplot(length_as_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_length = length_as_series.quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pad_length = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - padding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(len(encoded_texts))):\n",
    "    if len(encoded_texts[i]) < max_pad_length:\n",
    "        reviews.append(encoded_texts[i])\n",
    "        labels.append(1 if labels_as_list[i] == \"positive\" else 0)\n",
    "        \n",
    "assert len(reviews) == len(labels), \"The labels and feature lists should have the same time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "padded_dataset = []\n",
    "for i in tqdm(range(len(reviews))):\n",
    "    padded_dataset.append(pad_tensor(reviews[i], int(max_pad_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing - transforming lists to tensors and splitting dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the final dataset:\n",
    "X = torch.stack(padded_dataset)\n",
    "y = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42)\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train), torch.tensor(y_train)\n",
    "X_test, y_test = torch.tensor(X_test), torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(encoder.vocab)+1, 32)\n",
    "        self.lstm = nn.LSTM(32, 32, batch_first=True)\n",
    "        self.fc1 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.embedding(x)\n",
    "        x_, (h_n, c_n) = self.lstm(x_)\n",
    "        x_ = (x_[:, -1, :])\n",
    "        x_ = self.fc1(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(ds_test, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Net()\n",
    "#_ = classifier.embedding.weight_mu.data.normal_()\n",
    "device = torch.device('cpu')\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=.005)#0.002 dives 85% acc\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_bar = tqdm(range(10),\n",
    "                 desc=\"Training\",\n",
    "                 position=0,\n",
    "                 total=2)\n",
    "\n",
    "acc = 0\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    batch_bar = tqdm(enumerate(train_loader),\n",
    "                     desc=\"Epoch: {}\".format(str(epoch)),\n",
    "                     position=1,\n",
    "                     total=len(train_loader))\n",
    "    \n",
    "    for i, (datapoints, labels) in batch_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = classifier(datapoints.long())\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #acc = (preds.argmax(dim=1) == labels).float().mean().cpu().item()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            acc = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for  i, (datapoints_, labels_) in enumerate(test_loader):\n",
    "                    preds = classifier(datapoints_)\n",
    "                    acc += (preds.argmax(dim=1) == labels_).float().sum().cpu().item()\n",
    "            acc /= len(X_test)\n",
    "\n",
    "        batch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                              accuracy=\"{:.2f}\".format(acc),\n",
    "                              epoch=epoch)\n",
    "        batch_bar.update()\n",
    "\n",
    "        \n",
    "    epoch_bar.set_postfix(loss=loss.cpu().item(),\n",
    "                          accuracy=\"{:.2f}\".format(acc),\n",
    "                          epoch=epoch)\n",
    "    epoch_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
